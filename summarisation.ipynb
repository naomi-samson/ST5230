{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 991.5/991.5 kB 15.5 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          note_id  subject_id     hadm_id note_type  note_seq  \\\n",
      "0  10000032-RR-14    10000032  22595853.0        RR        14   \n",
      "1  10000032-RR-15    10000032  22595853.0        RR        15   \n",
      "2  10000032-RR-16    10000032  22595853.0        RR        16   \n",
      "3  10000032-RR-18    10000032         NaN        RR        18   \n",
      "4  10000032-RR-20    10000032         NaN        RR        20   \n",
      "\n",
      "             charttime            storetime   Examination  \\\n",
      "0  2180-05-06 21:19:00  2180-05-06 23:32:00         CHEST   \n",
      "1  2180-05-06 23:00:00  2180-05-06 23:26:00         LIVER   \n",
      "2  2180-05-07 09:55:00  2180-05-07 11:15:00           NaN   \n",
      "3  2180-06-03 12:46:00  2180-06-03 14:01:00    Ultrasound   \n",
      "4  2180-07-08 13:18:00  2180-07-08 14:15:00  Paracentesis   \n",
      "\n",
      "                                          Indication  \\\n",
      "0      with new onset ascites  // eval for infection   \n",
      "1          year-old female with cirrhosis, jaundice.   \n",
      "2   HCV cirrhosis c/b ascites, hiv on ART, h/o IV...   \n",
      "3   year old woman with cirrhosis, ascites despit...   \n",
      "4   year old woman with cirrhosis, ascites despit...   \n",
      "\n",
      "                                           Technique             Comparison  \\\n",
      "0                               Chest PA and lateral                  None.   \n",
      "1  Grey scale and color Doppler ultrasound images...                  None.   \n",
      "2  Ultrasound guided diagnostic and therapeutic p...  Abdominal ultrasound    \n",
      "3         Ultrasound guided therapeutic paracentesis                      .   \n",
      "4         Ultrasound guided therapeutic paracentesis       Paracentesis on    \n",
      "\n",
      "                                            Findings  \\\n",
      "0  There is no focal consolidation, pleural effus...   \n",
      "1  LIVER: The liver is coarsened and nodular in e...   \n",
      "2  Limited grayscale ultrasound imaging of the ab...   \n",
      "3  Limited grayscale ultrasound imaging of the ab...   \n",
      "4  Limited grayscale ultrasound imaging of the ab...   \n",
      "\n",
      "                                          Impression  \\\n",
      "0                  No acute cardiopulmonary process.   \n",
      "1  1. Nodular appearance of the liver compatible ...   \n",
      "2  Successful uncomplicated ultrasound guided dia...   \n",
      "3  Uneventful therapeutic paracentesis yielding 1...   \n",
      "4  4.75 L of slightly cloudy, blood tinged fluid ...   \n",
      "\n",
      "                                       combined_text  \\\n",
      "0  CHEST  with new onset ascites  // eval for inf...   \n",
      "1  LIVER  year-old female with cirrhosis, jaundic...   \n",
      "2  nan  HCV cirrhosis c/b ascites, hiv on ART, h/...   \n",
      "3  Ultrasound  year old woman with cirrhosis, asc...   \n",
      "4  Paracentesis  year old woman with cirrhosis, a...   \n",
      "\n",
      "                                      tokenized_text Gender  \n",
      "0  ['chest', 'new', 'onset', 'ascites', 'eval', '...    NaN  \n",
      "1  ['liver', 'female', 'cirrhosis', 'jaundice', '...      F  \n",
      "2  ['hcv', 'cirrhosis', 'c', 'b', 'ascites', 'hiv...    NaN  \n",
      "3  ['ultrasound', 'woman', 'cirrhosis', 'ascites'...      F  \n",
      "4  ['paracentesis', 'woman', 'cirrhosis', 'ascite...      F  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    CHEST  with new onset ascites  // eval for inf...\n",
       "1    LIVER  year-old female with cirrhosis, jaundic...\n",
       "2    nan  HCV cirrhosis c/b ascites, hiv on ART, h/...\n",
       "3    Ultrasound  year old woman with cirrhosis, asc...\n",
       "4    Paracentesis  year old woman with cirrhosis, a...\n",
       "Name: combined_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Preprocessed_radiology_data.xls\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    CHEST  with new onset ascites  // eval for inf...\n",
       "1    LIVER  year-old female with cirrhosis, jaundic...\n",
       "2    nan  HCV cirrhosis c/b ascites, hiv on ART, h/...\n",
       "3    Ultrasound  year old woman with cirrhosis, asc...\n",
       "4    Paracentesis  year old woman with cirrhosis, a...\n",
       "Name: combined_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['combined_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There is no focal consolidation, pleural effusion or pneumothorax. CHEST  with new onset ascites  // eval for infection Chest PA and lateral None.   Bilateral No acute cardiopulmonary process\n"
     ]
    }
   ],
   "source": [
    "def extractive_tfidf_summary(text, num_sentences=3):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Calculate cosine similarity between sentences\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # Sum the cosine similarity scores for each sentence\n",
    "    sentence_scores = cosine_sim.sum(axis=1)\n",
    "    \n",
    "    # Get indices of the top N sentences\n",
    "    top_sentence_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
    "    \n",
    "    # Select and return the top sentences\n",
    "    summary = '. '.join([sentences[i] for i in top_sentence_indices])\n",
    "    return summary\n",
    "\n",
    "# Example: Apply to one of the combined texts\n",
    "example_text = df['combined_text'][0]\n",
    "summary_tfidf = extractive_tfidf_summary(example_text)\n",
    "print(summary_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summarize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Example: Apply to one of the combined texts\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m summary_textrank \u001b[38;5;241m=\u001b[39m \u001b[43mextractive_textrank_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_textrank)\n",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m, in \u001b[0;36mextractive_textrank_summary\u001b[1;34m(text, num_sentences)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextractive_textrank_summary\u001b[39m(text, num_sentences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Gensim's summarize function does the sentence ranking automatically\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize\u001b[49m(text, word_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# You can adjust this parameter\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summarize' is not defined"
     ]
    }
   ],
   "source": [
    "def extractive_textrank_summary(text, num_sentences=3):\n",
    "    # Gensim's summarize function does the sentence ranking automatically\n",
    "    summary = summarize(text, word_count=100)  # You can adjust this parameter\n",
    "    return summary\n",
    "\n",
    "# Example: Apply to one of the combined texts\n",
    "summary_textrank = extractive_textrank_summary(example_text)\n",
    "print(summary_textrank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the pre-trained T5 model and tokenizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabstractive_t5_summary\u001b[39m(text):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Encode the input text and generate the output\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarize: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\naomi\\anaconda3\\envs\\d2l\\lib\\site-packages\\transformers\\utils\\import_utils.py:1736\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1736\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\naomi\\anaconda3\\envs\\d2l\\lib\\site-packages\\transformers\\utils\\import_utils.py:1724\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1722\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def abstractive_t5_summary(text):\n",
    "    # Encode the input text and generate the output\n",
    "    input_text = f\"summarize: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example: Apply to one of the combined texts\n",
    "summary_t5 = abstractive_t5_summary(example_text)\n",
    "print(summary_t5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
